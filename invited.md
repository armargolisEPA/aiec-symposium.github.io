---

title: Invited Speakers

---

Increasing enforcement efficiency  

Presentation 1  

Title: Are the inspections going to “waste”? A national field test of machine learning vs. expert judgement to improve EPA regulatory compliance  

Authors: Katherine Meckel (UC-San Diego), Jesse Buchsbaum (UChicago), Michael Greenstone (UChicago) 

Suggested presenter: Katherine Meckel (UC-San Diego) 

 

Abstract: Machine learning has the potential to improve targeting of regulatory inspections in theory, but this potential has been untested in practice. We directly test machine learning against human decision-making, first developing a model to predict severe violations of hazardous waste regulation, then conducting a field test of the model’s performance. The inspections chosen by the machine learning model identified 79% more severe violations than those selected by humans. These results provide evidence of the efficiency gains that machine learning can offer in regulatory compliance, highlighting the need to explore the adoption of such tools.  

Presentation 2  

Title: Using Machine Learning Models to Enhance Violation Detection of EPA Regulated Facilities  

Authors: Jesse Buchsbaum, Michael Greenstone, Rajat Kochhar, Olga Rostapshova (UChicago) 

Suggested presenter: Jesse Buchsbaum (University Of Chicago) 

Abstract: A predictive analytics model can make the targeting of inspections more efficient in terms of achieving higher violation discovery rates. We develop an ML model that focuses on violations related to provisions in the Clean Air Act (CAA) and National Pollutant Discharge Elimination System (NPDES). The project leverages extensive data on historical inspection, violation, enforcement, and compliance data to predict where violations are most likely to occur within the large and less inspected universe of EPA regulated facilities.  

Presentation 3  

Title: Improving Methane Emissions Monitoring and Enforcement using Predictive Analytics for Inspection Targeting  

Authors: Thomas Covert, Ludovica Gazze, Michael Greenstone, and Olga Rostapshova  

Suggested presenter: Ludovica Gazze (University of Warwick)  

Abstract: Regulating methane, a greenhouse gas with 80 times the short-term global warming potential of carbon dioxide, is at the forefront of the global agenda to address climate change. In the United States, methane emissions are particularly pernicious at oil and gas (O&G) facilities, from which an estimated 2.3% of gross natural gas production (13 billion kilograms of methane) leaks each year. We partnered with Colorado Department of Public Health and Environment (CDPHE) to build a supervised machine learning model to predict the likelihood that upstream O&G facilities in Colorado would leak natural gas. This model leveraged previously unlinked administrative data, O&G permitting and inspection histories, monthly O&G production data, meteorological conditions, oil and gas prices, and demographic data to predict unauthorized emissions.  Over the last two years, CDPHE has used the ML model output to target inspection sites; consequently, inspectors have observed a substantial increase in the number of emissions events they observed during inspections.  

Presentation 4  

Title: A Machine Learning Approach to Methane Emissions Mitigation in the Oil and Gas Industry  

Authors: Jiayang Wang (University of Texas at Austin), Selvaprabu Nadarajah (University of Illinois at Chicago), Jingfan Wang (Stanford University), Arvind P Ravikumar (University of Texas at Austin) 

Suggested Presenter: Arvind P Ravikumar (University of Texas at Austin) 

Abstract: Reducing methane emissions from the oil and gas sector is a key component of climate policy in the United States. Methane leaks across the supply chain are stochastic and intermittent, with a small number of sites (‘super-emitters’) responsible for a majority of emissions. Thus, cost-effective emissions reduction critically relies on effectively identifying the super-emitters from thousands of well sites and millions of miles of pipelines. Conventional approaches such as walking surveys using optical gas imaging technology are slow and time-consuming. In addition, several variables contribute to the formation of leaks such as infrastructure age, production, weather conditions, and maintenance practices. Here, we develop a machine learning algorithm to predict high-emitting sites that can be prioritized for follow-up repair. Such prioritization can significantly reduce the cost of surveys and increase emissions reductions compared to conventional approaches. Our results show that the algorithm using logistic regression performs the best out of several algorithms. The model achieved a 70% accuracy rate with a 57% recall and a 66% balanced accuracy rate. Compared to the conventional approach, the machine learning model reduced the time to achieve a 50% emissions mitigation target by 42%. Correspondingly, the mitigation cost reduced from $85/t CO2e to $49/t CO2e.  

 

Session 2: Big Data for Enforcement and Design of environmental regulations  

Presentation 5  

Title: Using Remote Sensing Technology to Reduce Heavy Duty Truck Emissions in California  

Authors: Fiona Burlig (UChicago), Ludovica Gazze (University of Warwick), Michael Greenstone (UChicago), and Olga Rostapshova (UChicago) 

Suggested presenter: Fiona Burlig (UChicago) 

Abstract: The transportation sector accounts for 29 percent of the total GHG emissions in the US. The California Air Resources Board (CARB) piloted an innovative technology, Portable Emissions Acquisition System (PEAQS), that can measure vehicle emissions in real time. This can allow regulators to identify high-emitting trucks and target enforcement actions more effectively. We designed an RCT to assess the impact of this technology to study whether enforcement programs guided by remote sensing technology can cost-effectively improve compliance and reduce emissions. High-emitting trucks were randomly assigned to either the control group or to one of two enforcement interventions: (1) a low-cost compliance documentation intervention targeted at individual trucks, and (2) a high-cost audit intervention targeted at truck fleets. In the low-cost treatment intervention, trucks were informed that they are likely in violation of California’s emissions standards and asked to provide compliance documentation. In the audit treatment, fleets with high-emitting trucks were subjected to comprehensive audits covering their entire truck population, with assured penalties if they do not prove compliance. We have enrolled 4,867 unique trucks in the low-cost documentation intervention and 70 unique fleets in the high-cost audit intervention. Data collection is expected to run for the next year. We also intend to measure changes in vehicle registrations across state borders, to observe the extent to which truck operators may employ regulatory evasion as an unintended consequence of the intervention.  

Presentation 6  

Title: Local Pollution Externalities from Driving: Evidence from Roadway Vehicle Sensors   

Authors: Andrew R. Waxman (University of Texas at Austin), Ruozi Song (World Bank), Rajat Kochhar (University of Chicago), Antonio M. Bento (University of Southern California) 

Suggested Presenter: Ruozi Song (World Bank) 

Abstract: This paper seeks to understand the localized effect of automobile congestion on air pollution. We causally identify this effect by leveraging air pollution sensors on Google Street View cars, combining the pollution readings with fine grain speed and vehicle density observations on Los Angeles Highways.  Our results indicate that pollution is highest at very low (<20km/h) and very high speeds (>60km/h), a result driven by lowered engine efficiency. We show that higher pollution occurs at very low and very high speeds due to lowered engine efficiency. Given the success to date of reducing pollution via tailpipe emission standards, we show that the magnitude of our effects points to the need to focus more attention by policymakers on vehicle speeds for further mitigation of health impacts from vehicle emissions. Our results have important implications for understanding human health effects of anti-congestion policies, speed limits and point to a need to better regulate sources of fine particulate matter pollution from tires and brakes.  
<h2 id="rance">Invited Speaker: <em><a href="https://www.cs.umd.edu/people/wcleavel">Rance Cleaveland</a></em></h2>
<p><img src="/assets/img/rance.jpg" alt="rance" class="img-responsive" style="max-width: 40%; float: left; border-radius: 3%; margin-right: 25px" /></p>

<p style="text-align: justify">Rance Cleaveland is Professor of Computer Science at the University of Maryland in College Park, w
here he also serves as Associate Dean of Research for the College of Computing, Mathematical and Natural Sciences.  
He has published widely in the areas of software verification and validation, formal methods, model checking, software 
specification formalisms, and verification tools.  Cleaveland received B.S. degrees in Mathematics and Computer Science 
from Duke University, and M.S. and Ph.D. degrees from Cornell University.  He is a Fellow of the IEEE and recently finished 
a four-year stint as Division Director of Computing and Communication Foundations at the US National Science Foundation.</p>

<h3 style="text-align: justify" id="keynote-abstract---download-slides">Query-Checking for Finite Linear-time Temporal Logic</h3>

<p>This talk addresses the following problem:  given a finite set of observations of system behavior, each observation 
itself being a finite sequence of system states, and a query consisting of a Finite Linear-Time Temporal Logic (LTL) 
formula with missing sub-formulas, compute missing the missing sub-formulas that make the formula true for all observations.  
This so-called query-checking problem has many applications in the analysis of time-series data, including server logs and 
financial trend data, as well as generally in system comprehension and system-specification mining.  The presentation first 
introduces Finite LTL, which is the well-known LTL logic of Pnueli interpreted over finite, rather than infinite, state sequences.  
It will then show how automata may be constructed from such formulas that recognize exactly the sequences making the formula true, 
and how these automata may be adapted to Finite LTL queries in which sub-formulas are missing.  A procedure is then described that 
uses the automaton representation of such a query to solve for the missing sub-formulas for a given a set of finite state sequences.  
Preliminary experimental results of a prototype implementation are also given.
This work is joint with Sam Huang.
</p>


<h2 id="degiacomo">Invited Speaker: <em><a href="https://www.cs.ox.ac.uk/people/giuseppe.degiacomo/">Giuseppe De Giacomo</a></em></h2>
<p><img src="/assets/img/degiacomo.jpeg" alt="degiacomo" class="img-responsive" style="max-width: 35%; float: left; border-radius: 3%; margin-right: 25px" /></p>

<p style="text-align: justify">Giuseppe De Giacomo is currently a Professor of Computer Science at the Department of 
Computer Science of the University of Oxford. For several years he has been a professor at the Department of Computer, 
Control and Management Engineering of the University of Roma "La Sapienza". His research activity has concerned theoretical, 
methodological, and practical aspects in different areas of AI and CS, most prominently Knowledge Representation, 
Reasoning about Actions, Generalized Planning, Autonomous Agents, Service Composition, Business Process Modeling, 
and Data Management and Integration. He is AAAI Fellow, ACM Fellow, and EurAI Fellow. He has got an ERC Advanced Grant 
for the project WhiteMech: White-box Self Programming Mechanisms (2019-2024). He has been the Program Chair of ECAI 2020. 
He is on the Board of EurAI.</p>

<h3 style="text-align: justify" id="keynote-abstract---download-slides">Linear-time Temporal Logics on Finite Traces</h3>
<p>In this talk we look at temporal logics on traces that are assumed to be finite, as typical of action planning in
Artificial Intelligence and of processes modeling in Business Process Management.  Having to deal with finite traces has 
been considered a sort of accident in much of the AI and BPM literature, and standard temporal logics on infinite traces 
have been hacked to fit this assumption. Only recently a specific interest in studying the impact of such an assumption 
has emerged.  We will survey linear temporal logics on finite traces, including LTLf, LDLf, and pure-past LTL, reviewing 
the main results on Reasoning, Verification, and Synthesis. The talk will also draw connections with work in AI planning, 
Non-Markovin Decision Processes, Reinforcement Learning, and BPM Declarative Process Modeling, that will then be elaborated 
on in separate talks by workshop participants.</p>

<h2 id="sheila">Invited Speaker: <em><a href="https://www.cs.toronto.edu/~sheila/">Sheila McIlraith </a></em></h2>
<p><img src="/assets/img/sheila.jpg" alt="degiacomo" class="img-responsive" style="max-width: 43%; float: left; border-radius: 3%; margin-right: 25px" /></p>

<p style="text-align: justify">Sheila McIlraith is a Professor in the Department of Computer Science at the University 
of Toronto, a Canada CIFAR AI Chair (Vector Institute), and an Associate Director at the Schwartz Reisman Institute for 
Technology and Society. Prior to joining U of T, McIlraith spent six years as a Research Scientist at Stanford University, 
and one year at Xerox PARC. McIlraith’s research is in the area of AI knowledge representation and reasoning, and machine 
learning where she currently studies sequential decision-making, broadly construed, with a focus on human-compatible AI. 
McIlraith is a Fellow of the ACM and the Association for the Advancement of Artificial Intelligence (AAAI). She and co-authors 
have been recognized with two test-of-time awards from the International Semantic Web Conference (ISWC) in 2011, and from the 
International Conference on Automated Planning and Scheduling (ICAPS) in 2022.</p>

<h3 style="text-align: justify" id="keynote-abstract---download-slides">Automated Planning with LTL: Techniques, Advances, and its Broad Applicability
</h3>
<p>Given a description of the dynamics of an environment, the initial state of the world, and a goal condition, 
automated symbolic planning generates a plan – typically a sequence of actions – that, when executed starting in the 
initial state, achieves the goal condition. Over the last 20+ years, research in automated planning has resulted in 
highly efficient algorithms for plan generation under varying models of the environment and goal conditions. 
In this talk, I’ll briefly review this literature, focusing on techniques for planning with temporally extended goals 
expressed in syntactic variants of LTL and regular expressions, interpreted over finite or infinite traces.  
I’ll show how planning in deterministic or nondeterministic systems with such temporally extended goals (or preferences) 
can be used as the computational core for a diverse set of tasks from web service composition to narrative understanding, 
goal recognition, automated diagnosis, and reactive synthesis. I’ll conclude with a brief discussion of how some of these 
same insights have been repurposed in the context of reinforcement learning, solving problems that otherwise could not be solved.</p>

<h2 id="sheila">Invited Speaker: <em><a href="http://www.inf.unibz.it/~montali/">Marco Montali</a> </em></h2>
<p><img src="/assets/img/marco.png" alt="degiacomo" class="img-responsive" style="max-width: 30%; float: left; border-radius: 3%; margin-right: 25px" /></p>

<p style="text-align: justify">Marco Montali is Full Professor in the Faculty of Engineering at the Free University of 
Bozen-Bolzano, Italy, where he also coordinates the MSc Program in Computational Data Science.  The Leitmotiv of his 
research is to develop novel, foundational and applied techniques grounded in artificial intelligence, formal methods, 
and data science to create intelligent agents and information systems that combine processes and data. 
He has served as PC Chair of BPM 2018, RuleML+RR 2019, ICPM 2020, and CBI 2021, as General Chair of ICPM 2022 and EDOC 
2022, and is steering committee member of the IEEE task force on process mining. He is co-author of more than 250 papers 
and recipient of 10 best paper awards and 2 test-of-time awards. He received the 2015 “Marco Somalvico” award, 
given by the Italian Association of Artificial Intelligence to the best under 35 Italian researcher advancing 
the state-of-the-art in artificial intelligence.

</p>

<h3 style="text-align: justify" id="keynote-abstract---download-slides">Declarative Process Management and mining: A Killer Application for LTLf
</h3>
<p>Business process management (BPM) is a discipline at the intersection between operations management, computer science, 
and software and systems engineering, whose grand goal is to support managers, analysts, and domain experts in the design, 
deployment, enactment, and continuous improvement of (work) processes within an organisation.  While processes are traditionally 
managed in a top-down manner, process mining techniques are revolusionatising BPM, making it possible to automatically discover 
process models, check conformance between expected and actual executions, monitor and formulate predictions, and ultimately 
continuously improve processes based on the factual event data recorded inside information systems.
In this talk, we focus on the widespread class of knowledge-intensive processes, which challenge the foundations of 
BPM and process mining, due to their inherent flexibility. We argue that such processes cannot be satisfactory modelled 
nor understood using conventional, procedural process modelling languages, but are instead best captured using a declarative 
approach based on temporal constraints. We ground our discussion on one of the most prominent languages in the declarative 
BPM spectrum: Declare. We show that Declare is naturally formalised in LTLf, and explain how the automata-theoretic characterisation 
of LTLf provides a solid, effective basis to elegantly solve a variety of central BPM/process mining tasks: model verification, enactment, anticipatory monitoring, and process discovery. In doing so, we touch on interesting recent extensions of LTLf dealing with data and uncertainty.
</p>


<h2 id="kristin">Invited Speaker: <em><a href="https://www.engineering.iastate.edu/people/profile/kyrozier/">Kristin Yvonne Rozier</a> </em></h2>
<p><img src="/assets/img/kristin.jpg" alt="kristin" class="img-responsive" style="max-width: 33%; float: left; border-radius: 3%; margin-right: 25px" /></p>

<p style="text-align: justify">Professor Kristin Yvonne Rozier heads the Laboratory for Temporal Logic in Aerospace 
Engineering at Iowa State University; previously she spent 14 years as a Research Scientist at NASA and three semesters 
as an Assistant Professor at the University of Cincinnati. She earned her Ph.D. from Rice University and B.S. and M.S. 
degrees from The College of William and Mary. Dr. Rozier's research focuses on automated techniques for the formal specification, 
validation, and verification of safety critical systems. Her primary research interests include: design-time checking of system 
logic and system requirements; runtime system health management; and safety and security analysis. 

Her advances in computation for the aerospace domain earned her many awards including: the NSF CAREER Award; the NASA 
Early Career Faculty Award; American Helicopter Society's Howard Hughes Award; Women in Aerospace Inaugural 
Initiative-Inspiration-Impact Award; two NASA Group Achievement Awards; two NASA Superior Accomplishment Awards; 
Lockheed Martin Space Operations Lightning Award; AIAA's Intelligent Systems Distinguished Service Award. 
She holds an endowed position as Building a World of Difference faculty fellow, is an Associate Fellow of AIAA, 
and is a Senior Member of IEEE, ACM, and SWE. Dr. Rozier has served on the NASA Formal Methods Symposium Steering 
Committee since working to found that conference in 2008.


</p>

<h3 style="text-align: justify" id="keynote-abstract---download-slides">On the Effectiveness of Mission-time Linear Temporal Logic (MLTL) in AI Applications 
</h3>
<p>Mission-time Linear Temporal Logic (MLTL) adds closed-interval integer bounds on the temporal operators of LTL, 
enabling unit-agnostic specification over finite traces. It is arguably the most-used variation of MTL, and the most-used 
subset of STL in industrial and AI applications. MLTL optimizes the trade-off between expressibility of a wide range of 
realistic requirements and the ability to author generic, easy-to-validate formulas. We highlight successful AI applications 
centered around MLTL requirements, including Robonaut2 and the NASA Lunar Gateway Vehicle System Manager. We overview advances 
in analyzing MLTL, explain the motivation driving these developments, and point out the gaps in the state of the art where 
there are needs for future work.</p>

<h2 id="ufuk">Invited Speaker: <em><a href="https://www.ae.utexas.edu/people/faculty/faculty-directory/topcu">Ufuk Topcu</a> </em></h2>
<p><img src="/assets/img/ufuk.jpg" alt="ufuk" class="img-responsive" style="max-width: 40%; float: left; border-radius: 3%; margin-right: 25px" /></p>

<p style="text-align: justify">Ufuk Topcu is an Associate Professor in the Department of Aerospace Engineerin at 
The University of Texas at Austin, where he directs the Autonomos Systems Group. Ufuk’s research focuses on the 
theoretical and algorithmic aspects of the design and verification of autonomous systems, typically at the intersection 
of formal methods, reinforcement learning, and control theory. He takes a relatively broad view on autonomy and tends 
to tackle abstract problems motivated by challenges cutting across multiple applications of autonomy. 

</p>

<h3 style="text-align: justify" id="keynote-abstract---download-slides">Automaton-Based Representations of Task Knowledge from Generative Language Models
</h3>
<p> Automaton-based representations of task knowledge play an important role in control, planning, and learning for 
sequential decision-making. However, obtaining the high-level task knowledge required to build such automata is 
often difficult. Meanwhile, large-scale generative language models can automatically generate relevant task knowledge. 
However, the textual outputs from generative langauge models cannot be formally verified or used for sequential 
decision-making. I will discuss our recent exploration into constructing automaton-based encodings of high-level 
task knowledge from a brief natural-language description of the task goal. I will also discuss how the outcomes may 
be integrated with formal verification and task-guided reinforcement learning. </p>

<h2 id="moshe">Invited Speaker: <em><a href="https://www.cs.rice.edu/~vardi/">Moshe Vardi</a> </em></h2>
<p><img src="/assets/img/moshe.jpg" alt="ufuk" class="img-responsive" style="max-width: 35%; float: left; border-radius: 3%; margin-right: 25px" /></p>

<p style="text-align: justify">Moshe Y. Vardi is a University Professor, and the George Distinguished Service 
Professor in Computational Engineering at Rice University.  He is the author and co-author of over 700 papers, 
as well as two books.  He is the recipient of several scientific awards, is a fellow of several societies, 
and a member of several honorary academies. He holds eight honorary doctorates.  He is a Senior Editor of 
Communications of the ACM, the premier publication in computing, focusing on societal impact of information technology.

</p>

<h3 style="text-align: justify" id="keynote-abstract---download-slides">From Infinite to Finite Horizons
</h3>
<p> Linear Temporal Logic (LTL), proposed in 1977 by Amir Pnueli for reasoning about ongoing programs, was defined over 
infinite traces. The motivation for this was the desire to model arbitrarily long
computations. While this approach has been highly successful in the context of model checking, it has been less 
successful in the context of reactive synthesis, due to the chalenging algorithmics of infinite-horizon temporal 
synthesis. In this talk we show that focusing on finite-horizon temporal synthesis offers enough algorithmic 
advantages to compensate for the loss in expressiveness. In fact, finite-horizon reasonings is useful even in the 
context of infinite-horizon applications.
 </p>

